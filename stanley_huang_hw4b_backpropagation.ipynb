{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanleyhuang12/ds542-deep-learning/blob/main/stanley_huang_hw4b_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7e3e76",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ca7e3e76"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(\"backpropagation.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d692ffcd",
      "metadata": {
        "id": "d692ffcd"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Based on the notebooks in the UDL book.\n",
        "\n",
        "In this notebook, you will explore backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f39c92e-2a61-444f-88ef-62ab49b9de49",
      "metadata": {
        "id": "5f39c92e-2a61-444f-88ef-62ab49b9de49"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap07/7_1_Backpropagation_in_Toy_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "909ab76f-03b6-416e-8441-099b352ceeea",
      "metadata": {
        "id": "909ab76f-03b6-416e-8441-099b352ceeea"
      },
      "source": [
        "## Part 1: Backpropagation in Toy Model\n",
        "\n",
        "This section computes the derivatives of the toy function discussed in section 7.3 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "477c25c1-e2e4-4a37-bc77-f3ca6a9e9516",
      "metadata": {
        "id": "477c25c1-e2e4-4a37-bc77-f3ca6a9e9516"
      },
      "source": [
        "We're going to investigate how to take the derivatives of functions where one operation is composed with another, which is composed with a third and so on.  For example, consider the model:\n",
        "\n",
        "\\begin{equation}\n",
        "     \\text{f}[x,\\boldsymbol\\phi] = \\beta_3+\\omega_3\\cdot\\cos\\Bigl[\\beta_2+\\omega_2\\cdot\\exp\\bigl[\\beta_1+\\omega_1\\cdot\\sin[\\beta_0+\\omega_0x]\\bigr]\\Bigr],\n",
        "\\end{equation}\n",
        "\n",
        "with parameters $\\boldsymbol\\phi=\\{\\beta_0,\\omega_0,\\beta_1,\\omega_1,\\beta_2,\\omega_2,\\beta_3,\\omega_3\\}$.<br>\n",
        "\n",
        "This is a composition of the functions $\\cos[\\bullet],\\exp[\\bullet],\\sin[\\bullet]$.   I chose these just because you probably already know the derivatives of these functions:\n",
        "\n",
        "\\begin{align}\n",
        " \\frac{\\partial \\cos[z]}{\\partial z} = -\\sin[z] \\quad\\quad \\frac{\\partial \\exp[z]}{\\partial z} = \\exp[z] \\quad\\quad \\frac{\\partial \\sin[z]}{\\partial z} = \\cos[z].\n",
        "\\end{align}\n",
        "\n",
        "Suppose that we have a least squares loss function:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\ell_i = (\\text{f}[x_i,\\boldsymbol\\phi]-y_i)^2,\n",
        "\\end{equation*}\n",
        "\n",
        "Assume that we know the current values of $\\beta_{0},\\beta_{1},\\beta_{2},\\beta_{3},\\omega_{0},\\omega_{1},\\omega_{2},\\omega_{3}$, $x_i$ and $y_i$. We could obviously calculate $\\ell_i$.   But we also want to know how $\\ell_i$ changes when we make a small change to $\\beta_{0},\\beta_{1},\\beta_{2},\\beta_{3},\\omega_{0},\\omega_{1},\\omega_{2}$, or $\\omega_{3}$.  In other words, we want to compute the eight derivatives:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\ell_i}{\\partial \\beta_{0}}, \\quad \\frac{\\partial \\ell_i}{\\partial \\beta_{1}}, \\quad \\frac{\\partial \\ell_i}{\\partial \\beta_{2}}, \\quad \\frac{\\partial \\ell_i }{\\partial \\beta_{3}},  \\quad \\frac{\\partial \\ell_i}{\\partial \\omega_{0}}, \\quad \\frac{\\partial \\ell_i}{\\partial \\omega_{1}}, \\quad \\frac{\\partial \\ell_i}{\\partial \\omega_{2}},  \\quad\\text{and} \\quad \\frac{\\partial \\ell_i}{\\partial \\omega_{3}}.\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a7215f-bbf6-48d4-92c5-79d5d3d58134",
      "metadata": {
        "id": "26a7215f-bbf6-48d4-92c5-79d5d3d58134"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21498078-cafc-49ae-9fa9-3f92b46b4398",
      "metadata": {
        "id": "21498078-cafc-49ae-9fa9-3f92b46b4398"
      },
      "source": [
        "Let's first define the original function for $y$ and the loss term:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff8a4ee-105d-4266-945e-ad10b5ea55ec",
      "metadata": {
        "id": "cff8a4ee-105d-4266-945e-ad10b5ea55ec"
      },
      "outputs": [],
      "source": [
        "def fn(x, beta0, beta1, beta2, beta3, omega0, omega1, omega2, omega3):\n",
        "  return beta3+omega3 * np.cos(beta2 + omega2 * np.exp(beta1 + omega1 * np.sin(beta0 + omega0 * x)))\n",
        "\n",
        "def loss(x, y, beta0, beta1, beta2, beta3, omega0, omega1, omega2, omega3):\n",
        "  diff = fn(x, beta0, beta1, beta2, beta3, omega0, omega1, omega2, omega3) - y\n",
        "  return diff * diff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e443a253-55e7-46a3-bc37-44e327926815",
      "metadata": {
        "id": "e443a253-55e7-46a3-bc37-44e327926815"
      },
      "source": [
        "Now we'll choose some values for the betas and the omegas and x and compute the output of the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b1d8e5-ba07-4047-9657-a5f3c2987bed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55b1d8e5-ba07-4047-9657-a5f3c2987bed",
        "outputId": "57c5c78f-5f42-493c-843f-fb25422da6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l_i=0.139\n"
          ]
        }
      ],
      "source": [
        "beta0 = 1.0; beta1 = 2.0; beta2 = -3.0; beta3 = 0.4\n",
        "omega0 = 0.1; omega1 = -0.4; omega2 = 2.0; omega3 = 3.0\n",
        "x = 2.3; y = 2.0\n",
        "l_i_func = loss(x,y,beta0,beta1,beta2,beta3,omega0,omega1,omega2,omega3)\n",
        "print('l_i=%3.3f'%l_i_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ebb99b-23d9-4161-a9d8-a212ac38db53",
      "metadata": {
        "id": "e6ebb99b-23d9-4161-a9d8-a212ac38db53"
      },
      "source": [
        "# Computing derivatives by hand\n",
        "\n",
        "We could compute expressions for the derivatives by hand and write code to compute them directly but some have very complex expressions, even for this relatively simple original equation. For example:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\ell_i}{\\partial \\omega_{0}} &=& -2 \\left( \\beta_3+\\omega_3\\cdot\\cos\\Bigl[\\beta_2+\\omega_2\\cdot\\exp\\bigl[\\beta_1+\\omega_1\\cdot\\sin[\\beta_0+\\omega_0\\cdot x_i]\\bigr]\\Bigr]-y_i\\right)\\nonumber \\\\\n",
        "&&\\hspace{0.5cm}\\cdot \\omega_1\\omega_2\\omega_3\\cdot x_i\\cdot\\cos[\\beta_0+\\omega_0 \\cdot x_i]\\cdot\\exp\\Bigl[\\beta_1 + \\omega_1 \\cdot \\sin[\\beta_0+\\omega_0\\cdot x_i]\\Bigr]\\nonumber\\\\\n",
        "&& \\hspace{1cm}\\cdot \\sin\\biggl[\\beta_2+\\omega_2\\cdot \\exp\\Bigl[\\beta_1 + \\omega_1 \\cdot \\sin[\\beta_0+\\omega_0\\cdot x_i]\\Bigr]\\biggr].\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cc5b05-1e53-46df-960e-d8f78fea89ce",
      "metadata": {
        "id": "c1cc5b05-1e53-46df-960e-d8f78fea89ce"
      },
      "outputs": [],
      "source": [
        "dldbeta3_func = 2 * (beta3 +omega3 * np.cos(beta2 + omega2 * np.exp(beta1+omega1 * np.sin(beta0+omega0 * x)))-y)\n",
        "dldomega0_func = -2 *(beta3 +omega3 * np.cos(beta2 + omega2 * np.exp(beta1+omega1 * np.sin(beta0+omega0 * x)))-y) * \\\n",
        "              omega1 * omega2 * omega3 * x * np.cos(beta0 + omega0 * x) * np.exp(beta1 +omega1 * np.sin(beta0 + omega0 * x)) *\\\n",
        "              np.sin(beta2 + omega2 * np.exp(beta1+ omega1* np.sin(beta0+omega0 * x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc1922c-c905-4bcc-90a0-6d873b7c0c84",
      "metadata": {
        "id": "efc1922c-c905-4bcc-90a0-6d873b7c0c84"
      },
      "source": [
        "Let's make sure this is correct using finite differences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f53eb1-a2c7-4f3c-9ba2-429a8f21a3c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2f53eb1-a2c7-4f3c-9ba2-429a8f21a3c7",
        "outputId": "e91fe2f6-334e-4b30-e739-8d35ae597959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dydomega0: Function value = 5.246, Finite difference value = 5.246\n"
          ]
        }
      ],
      "source": [
        "dldomega0_fd = (loss(x,y,beta0,beta1,beta2,beta3,omega0+0.00001,omega1,omega2,omega3)-loss(x,y,beta0,beta1,beta2,beta3,omega0,omega1,omega2,omega3))/0.00001\n",
        "\n",
        "print('dydomega0: Function value = %3.3f, Finite difference value = %3.3f'%(dldomega0_func,dldomega0_fd))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ef2cfb-0328-474a-9a44-9855dd8a2079",
      "metadata": {
        "id": "f6ef2cfb-0328-474a-9a44-9855dd8a2079"
      },
      "source": [
        "The code to calculate $\\partial l_i/ \\partial \\omega_0$ is a bit of a nightmare.  It's easy to make mistakes, and you can see that some parts of it are repeated (for example, the $\\sin[\\bullet]$ term), which suggests some kind of redundancy in the calculations.  The goal of this practical is to compute the derivatives in a much simpler way.  There will be three steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019f6d56-4502-4712-923d-7d378e5dcef7",
      "metadata": {
        "id": "019f6d56-4502-4712-923d-7d378e5dcef7"
      },
      "source": [
        "**Step 1:** Write the original equations as a series of intermediate calculations.\n",
        "\n",
        "\\begin{align}\n",
        "f_{0} &=& \\beta_{0} + \\omega_{0} x_i\\nonumber\\\\\n",
        "h_{1} &=& \\sin[f_{0}]\\nonumber\\\\\n",
        "f_{1} &=& \\beta_{1} + \\omega_{1}h_{1}\\nonumber\\\\\n",
        "h_{2} &=& \\exp[f_{1}]\\nonumber\\\\\n",
        "f_{2} &=& \\beta_{2} + \\omega_{2} h_{2}\\nonumber\\\\\n",
        "h_{3} &=& \\cos[f_{2}]\\nonumber\\\\\n",
        "f_{3} &=& \\beta_{3} + \\omega_{3}h_{3}\\nonumber\\\\\n",
        "l_i &=& (f_3-y_i)^2\n",
        "\\end{align}\n",
        "\n",
        "and compute and store the values of all of these intermediate values.  We'll need them to compute the derivatives.<br>  This is called the **forward pass**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f724fda",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4f724fda"
      },
      "source": [
        "## Question 1.1\n",
        "\n",
        "Complete each line to compute the f_k and h_k terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1aa05ac-9570-4961-bd75-360586edc419",
      "metadata": {
        "id": "f1aa05ac-9570-4961-bd75-360586edc419",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# TODO compute all the f_k and h_k terms\n",
        "# Complete all the lines below\n",
        "\n",
        "f0 = beta0 + omega0 * x\n",
        "h1 = np.sin(f0)\n",
        "f1 = beta1 + omega1 * h1\n",
        "h2 = np.exp(f1)\n",
        "f2 = beta2 + omega2 * h2\n",
        "h3 = np.cos(f2)\n",
        "f3 = beta3 + omega3 * h3\n",
        "l_i = (f3 - y) ** 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c383482-f559-4706-afca-6ecd8dab69a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "0c383482-f559-4706-afca-6ecd8dab69a3",
        "outputId": "5bb3310f-fd68-40e2-de06-5dc664a207ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f0: true value = 1.230, your value = 1.230\n",
            "h1: true value = 0.942, your value = 0.942\n",
            "f1: true value = 1.623, your value = 1.623\n",
            "h2: true value = 5.068, your value = 5.068\n",
            "f2: true value = 7.137, your value = 7.137\n",
            "h3: true value = 0.657, your value = 0.657\n",
            "f3: true value = 2.372, your value = 2.372\n",
            "l_i original = 0.139, l_i from forward pass = 0.139\n"
          ]
        }
      ],
      "source": [
        "# Let's check we got that right:\n",
        "print(\"f0: true value = %3.3f, your value = %3.3f\"%(1.230, f0))\n",
        "print(\"h1: true value = %3.3f, your value = %3.3f\"%(0.942, h1))\n",
        "print(\"f1: true value = %3.3f, your value = %3.3f\"%(1.623, f1))\n",
        "print(\"h2: true value = %3.3f, your value = %3.3f\"%(5.068, h2))\n",
        "print(\"f2: true value = %3.3f, your value = %3.3f\"%(7.137, f2))\n",
        "print(\"h3: true value = %3.3f, your value = %3.3f\"%(0.657, h3))\n",
        "print(\"f3: true value = %3.3f, your value = %3.3f\"%(2.372, f3))\n",
        "print(\"l_i original = %3.3f, l_i from forward pass = %3.3f\"%(l_i_func, l_i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59678727",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "59678727"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3e870a-da75-4f7b-a4bb-7984e2564652",
      "metadata": {
        "id": "8b3e870a-da75-4f7b-a4bb-7984e2564652"
      },
      "source": [
        "**Step 2:** Compute the derivatives of $\\ell_i$ with respect to the intermediate quantities that we just calculated, but in reverse order:\n",
        "\n",
        "\\begin{align}\n",
        "\\quad \\frac{\\partial \\ell_i}{\\partial f_3}, \\quad \\frac{\\partial \\ell_i}{\\partial h_3}, \\quad \\frac{\\partial \\ell_i}{\\partial f_2}, \\quad\n",
        "\\frac{\\partial \\ell_i}{\\partial h_2}, \\quad \\frac{\\partial \\ell_i}{\\partial f_1}, \\quad \\frac{\\partial \\ell_i}{\\partial h_1},  \\quad\\text{and} \\quad \\frac{\\partial \\ell_i}{\\partial f_0}.\n",
        "\\end{align}\n",
        "\n",
        "The first of these derivatives is straightforward:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{3}} = 2 (f_3-y).\n",
        "\\end{equation}\n",
        "\n",
        "The second derivative can be calculated using the chain rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\ell_i}{\\partial h_{3}} =\\frac{\\partial f_{3}}{\\partial h_{3}} \\frac{\\partial \\ell_i}{\\partial f_{3}} .\n",
        "\\end{equation}\n",
        "\n",
        "The left-hand side asks how $\\ell_i$ changes when $h_{3}$ changes.  The right-hand side says we can decompose this into (i) how $\\ell_i$ changes when $f_{3}$ changes and how $f_{3}$ changes when $h_{3}$ changes.  So you get a chain of events happening:  $h_{3}$ changes $f_{3}$, which changes $\\ell_i$, and the derivatives represent the effects of this chain.  Notice that we computed the first of these derivatives already and is  $2 (f_3-y)$. We calculated $f_{3}$ in step 1.  The second term is the derivative of $\\beta_{3} + \\omega_{3}h_{3}$ with respect to $h_3$ which is simply $\\omega_3$.  \n",
        "\n",
        "We can continue in this way, computing the derivatives of the output with respect to these intermediate quantities:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{2}} &=& \\frac{\\partial h_{3}}{\\partial f_{2}}\\left(\n",
        "\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\n",
        "\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_{2}} &=& \\frac{\\partial f_{2}}{\\partial h_{2}}\\left(\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}}\\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{1}} &=& \\frac{\\partial h_{2}}{\\partial f_{1}}\\left( \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_{1}} &=& \\frac{\\partial f_{1}}{\\partial h_{1}}\\left(\\frac{\\partial h_{2}}{\\partial f_{1}} \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{0}} &=& \\frac{\\partial h_{1}}{\\partial f_{0}}\\left(\\frac{\\partial f_{1}}{\\partial h_{1}}\\frac{\\partial h_{2}}{\\partial f_{1}} \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right).\n",
        "\\end{align}\n",
        "\n",
        "In each case, we have already computed all of the terms except the last one in the previous step, and the last term is simple to evaluate.  This is called the **backward pass**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0365904a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0365904a"
      },
      "source": [
        "## Question 1.2\n",
        "\n",
        "Compute the derivatives of the output with respect to the intermediate computations $h_k$ and $f_k$ (i.e, run the backward pass). We've done the first two for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2967aa8-0b13-4258-9c8b-767275379421",
      "metadata": {
        "id": "a2967aa8-0b13-4258-9c8b-767275379421",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# TODO -- Compute the derivatives of the output with respect\n",
        "# to the intermediate computations h_k and f_k (i.e, run the backward pass)\n",
        "# I've done the first two for you.  You replace the code below:\n",
        "dldf3 = 2* (f3 - y)\n",
        "dldh3 = omega3 * dldf3\n",
        "# Replace the code below\n",
        "dldf2 = dldh3 * -np.sin(f2)\n",
        "dldh2 = omega2 * dldf2\n",
        "dldf1 = np.exp(f1) * dldh2\n",
        "dldh1 = dldf1 * omega1\n",
        "dldf0 = np.cos(f0) * dldh1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099cf78a-2782-4f21-8a8c-f877aceeec11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "099cf78a-2782-4f21-8a8c-f877aceeec11",
        "outputId": "9503e301-0aea-41b0-a9a4-a5cf56180104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dldf3: true value = 0.745, your value = 0.745\n",
            "dldh3: true value = 2.234, your value = 2.234\n",
            "dldf2: true value = -1.683, your value = -1.683\n",
            "dldh2: true value = -3.366, your value = -3.366\n",
            "dldf1: true value = -17.060, your value = -17.060\n",
            "dldh1: true value = 6.824, your value = 6.824\n",
            "dldf0: true value = 2.281, your value = 2.281\n"
          ]
        }
      ],
      "source": [
        "# Let's check we got that right\n",
        "print(\"dldf3: true value = %3.3f, your value = %3.3f\"%(0.745, dldf3))\n",
        "print(\"dldh3: true value = %3.3f, your value = %3.3f\"%(2.234, dldh3))\n",
        "print(\"dldf2: true value = %3.3f, your value = %3.3f\"%(-1.683, dldf2))\n",
        "print(\"dldh2: true value = %3.3f, your value = %3.3f\"%(-3.366, dldh2))\n",
        "print(\"dldf1: true value = %3.3f, your value = %3.3f\"%(-17.060, dldf1))\n",
        "print(\"dldh1: true value = %3.3f, your value = %3.3f\"%(6.824, dldh1))\n",
        "print(\"dldf0: true value = %3.3f, your value = %3.3f\"%(2.281, dldf0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb1bb8b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5bb1bb8b"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563fadd0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "563fadd0"
      },
      "source": [
        "## Question 1.3\n",
        "\n",
        "Calculate the final derivatives with respect to the beta and omega terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc06942-fbf7-4bd7-905a-2201255c6367",
      "metadata": {
        "id": "8fc06942-fbf7-4bd7-905a-2201255c6367",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# TODO -- Calculate the final derivatives with respect to the beta and omega terms\n",
        "\n",
        "dldbeta3 = dldf3\n",
        "dldomega3 = dldf3 * h3\n",
        "dldbeta2 = dldf2\n",
        "dldomega2 = dldf2 * h2\n",
        "dldbeta1 = dldf1\n",
        "dldomega1 = dldf1 * h1\n",
        "dldbeta0 = dldf0\n",
        "dldomega0 = dldf0 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee90792-24f1-4adf-99ad-91f200c319f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "3ee90792-24f1-4adf-99ad-91f200c319f2",
        "outputId": "255470f5-fa73-4abb-8ac9-1d7758a7e479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dldbeta3: Your value = 0.745, True value = 0.745\n",
            "dldomega3: Your value = 0.489, True value = 0.489\n",
            "dldbeta2: Your value = -1.683, True value = -1.683\n",
            "dldomega2: Your value = -8.530, True value = -8.530\n",
            "dldbeta1: Your value = -17.060, True value = -17.060\n",
            "dldomega1: Your value = -16.079, True value = -16.079\n",
            "dldbeta0: Your value = 2.281, True value = 2.281\n",
            "dldomega0: Your value = 5.246, Function value = 5.246, Finite difference value = 5.246\n"
          ]
        }
      ],
      "source": [
        "# Let's check we got them right\n",
        "print('dldbeta3: Your value = %3.3f, True value = %3.3f'%(dldbeta3, 0.745))\n",
        "print('dldomega3: Your value = %3.3f, True value = %3.3f'%(dldomega3, 0.489))\n",
        "print('dldbeta2: Your value = %3.3f, True value = %3.3f'%(dldbeta2, -1.683))\n",
        "print('dldomega2: Your value = %3.3f, True value = %3.3f'%(dldomega2, -8.530))\n",
        "print('dldbeta1: Your value = %3.3f, True value = %3.3f'%(dldbeta1, -17.060))\n",
        "print('dldomega1: Your value = %3.3f, True value = %3.3f'%(dldomega1, -16.079))\n",
        "print('dldbeta0: Your value = %3.3f, True value = %3.3f'%(dldbeta0, 2.281))\n",
        "print('dldomega0: Your value = %3.3f, Function value = %3.3f, Finite difference value = %3.3f'%(dldomega0, dldomega0_func, dldomega0_fd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf954d2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8bf954d2"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1.3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af28da6f-7514-49ae-a98e-0c8d7092de74",
      "metadata": {
        "id": "af28da6f-7514-49ae-a98e-0c8d7092de74"
      },
      "source": [
        "Using this method, we can compute the derivatives quite easily without needing to compute very complicated expressions.  In the next section, we'll apply this same method to a deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdb6ddf-7594-4cb5-97aa-3c07eeefd7de",
      "metadata": {
        "id": "8bdb6ddf-7594-4cb5-97aa-3c07eeefd7de"
      },
      "source": [
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6543aa-e606-4308-87ba-f690eca61842",
      "metadata": {
        "id": "1b6543aa-e606-4308-87ba-f690eca61842"
      },
      "source": [
        "## Part 2: Backpropagation\n",
        "\n",
        "This section runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d763a4-2c7f-4d85-a67b-863ae409d9af",
      "metadata": {
        "id": "01d763a4-2c7f-4d85-a67b-863ae409d9af"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd2432b-04ab-48c7-ae89-856903e9a9fc",
      "metadata": {
        "id": "8dd2432b-04ab-48c7-ae89-856903e9a9fc"
      },
      "source": [
        "First let's define a neural network in the form of equation 7.17 from the book but parameterized in the number of layers, $K$, neurons per layer, $D$, and input and output dimensions, $D_i$ and $D_o$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a31c2ac7",
      "metadata": {
        "id": "a31c2ac7"
      },
      "source": [
        "We'll just choose the weights and biases randomly for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b93c416-3903-4d17-b552-a8556b5ef340",
      "metadata": {
        "id": "7b93c416-3903-4d17-b552-a8556b5ef340"
      },
      "outputs": [],
      "source": [
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Number of hidden layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 6\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layers\n",
        "all_weights[0] = np.random.normal(size=(D, D_i))\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
        "all_biases[0] = np.random.normal(size =(D,1))\n",
        "all_biases[-1]= np.random.normal(size =(D_o,1))\n",
        "\n",
        "# Create intermediate layers\n",
        "for layer in range(1,K):\n",
        "  all_weights[layer] = np.random.normal(size=(D,D))\n",
        "  all_biases[layer] = np.random.normal(size=(D,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9d8d31-39d7-4533-a53b-476c031c3024",
      "metadata": {
        "id": "2b9d8d31-39d7-4533-a53b-476c031c3024"
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800fc319",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "800fc319"
      },
      "source": [
        "## Question 2.1\n",
        "\n",
        "Complete the function below to compute the output of the network.\n",
        "\n",
        "The weight matrices $\\boldsymbol\\Omega_{0\\ldots K}$ are the entries of the list `all_weights` and the biases $\\boldsymbol\\beta_{0\\ldots K}$ are the entries of the list `all_biases`.\n",
        "\n",
        "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad5cbb30-0b82-45c6-bdc3-fa7be8aabc41",
      "metadata": {
        "id": "ad5cbb30-0b82-45c6-bdc3-fa7be8aabc41",
        "tags": [
          "otter_answer_cell"
        ]
      },
      "outputs": [],
      "source": [
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "\n",
        "  # Retrieve number of layers using global variable 'K'\n",
        "  K = len(all_weights) -1\n",
        "\n",
        "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
        "  # and the activations in a second list \"all_h\".\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  #For convenience, we'll set\n",
        "  # all_h[0] to be the input, and all_f[K] will be the output\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
        "  for layer in range(K):\n",
        "      # Update preactivations and activations at this layer according to eqn 7.17\n",
        "      # Remember to use np.matmul for matrix multiplications\n",
        "      # TODO -- Replace the lines below\n",
        "      all_f[layer] = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]\n",
        "\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  # Compute the output from the last hidden layer\n",
        "  # TODO -- Replace the line below\n",
        "  all_f[K] = np.matmul(all_weights[K], all_h[K]) + all_biases[K]\n",
        "\n",
        "  # Retrieve the output\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a404287f-6e6a-4625-afad-b4617a6d6089",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a404287f-6e6a-4625-afad-b4617a6d6089"
      },
      "source": [
        "Now let's run our random network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e958c309-6213-4e65-a0a2-2d14bd8ee932",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e958c309-6213-4e65-a0a2-2d14bd8ee932",
        "outputId": "a38cc880-96db-4c40-f612-a43a649d4ff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True output = 1.907, Your answer = 1.907\n"
          ]
        }
      ],
      "source": [
        "# Define input\n",
        "net_input = np.ones((D_i,1)) * 1.2\n",
        "# Compute network output\n",
        "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35871ea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c35871ea"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a522246-44cc-49fc-bedc-4726536e942a",
      "metadata": {
        "id": "4a522246-44cc-49fc-bedc-4726536e942a"
      },
      "source": [
        "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b980611-d66a-427c-8a5d-9b2d8b3e3b4c",
      "metadata": {
        "id": "8b980611-d66a-427c-8a5d-9b2d8b3e3b4c"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "    return 2*(net_output -y);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6865d333-72ce-43fe-9b28-107b71dd3029",
      "metadata": {
        "id": "6865d333-72ce-43fe-9b28-107b71dd3029",
        "outputId": "150295d9-14ab-4c3f-e14f-6f82774f74f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = [[20.]] Loss = 327.371\n"
          ]
        }
      ],
      "source": [
        "y = np.ones((D_o,1)) * 20.0\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(f\"y = {y} Loss = {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3b16d68",
      "metadata": {
        "id": "f3b16d68"
      },
      "source": [
        "We'll define the indicator function as the derivative of ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f213cea7-d127-4ae2-a207-e6263e746a92",
      "metadata": {
        "id": "f213cea7-d127-4ae2-a207-e6263e746a92"
      },
      "outputs": [],
      "source": [
        "# We'll need the indicator function\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>0] = 1\n",
        "  x_in[x_in<=0] = 0\n",
        "  return x_in"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c08b88",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "18c08b88"
      },
      "source": [
        "## Question 2.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f331d7-cc33-4bae-97a6-949250e69a09",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "62f331d7-cc33-4bae-97a6-949250e69a09"
      },
      "source": [
        "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb410911",
      "metadata": {
        "tags": [
          "otter_answer_cell"
        ],
        "id": "eb410911"
      },
      "outputs": [],
      "source": [
        "# Main backward pass routine\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "\n",
        "  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "\n",
        "  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "\n",
        "  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
        "\n",
        "  # Compute derivatives of the loss with respect to the network output\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
        "\n",
        "  # Now work backwards through the network\n",
        "  for layer in range(K,-1,-1):\n",
        "    # TODO Calculate the derivatives of the loss with respect to the biases at\n",
        "    # layer from all_dl_df[layer]. (eq 7.22)\n",
        "    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
        "    # COMPLETE THIS LINE\n",
        "    all_dl_dbiases[layer] = all_dl_df[layer]\n",
        "    # TODO Calculate the derivatives of the loss with respect to the weights at\n",
        "    # layer from all_dl_df[layer] and all_h[layer] (eq 7.23)\n",
        "    # Don't forget to use np.matmul\n",
        "    # COMPLETE THIS LINE\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n",
        "\n",
        "    # TODO: calculate the derivatives of the loss with respect to the activations from weight and derivatives of next preactivations (second part of last line of eq 7.25)\n",
        "    # COMPLETE THIS LINE\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n",
        "\n",
        "\n",
        "    if layer > 0:\n",
        "      # TODO Calculate the derivatives of the loss with respect to the pre-activation f (use derivative of ReLu function, first part of last line of eq. 7.25)\n",
        "      # COMPLETE THIS LINE\n",
        "      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_dl_dh[layer]\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d97517-3136-4d2a-a14a-cfc6d448bcb1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "16d97517-3136-4d2a-a14a-cfc6d448bcb1"
      },
      "outputs": [],
      "source": [
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d816bd8-180c-4876-b38c-7106a0802dd8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0d816bd8-180c-4876-b38c-7106a0802dd8",
        "outputId": "c80422f6-91a6-4205-eb00-c8a6c4ce76cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[ -0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[  0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [-0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [ 0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-0.   ]\n",
            " [-5.212]\n",
            " [-0.   ]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-5.212]\n",
            " [ 0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 5, derivatives from backprop:\n",
            "[[-36.187]]\n",
            "Bias 5, derivatives from finite differences\n",
            "[[-36.187]]\n",
            "Success!  Derivatives match.\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Make space for derivatives computed by finite differences\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# Let's test if we have the derivatives right using finite differences\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Test the dervatives of the bias vectors\n",
        "for layer in range(K+1):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    # Take copy of biases  We'll change one element each time\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    raise RuntimeError(\"Failure!  Derivatives different.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e33222",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f8e33222",
        "outputId": "019605c2-203c-4503-c0a8-60edd2dd2369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 5, derivatives from backprop:\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Weight 5, derivatives from finite differences\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Success!  Derivatives match.\n"
          ]
        }
      ],
      "source": [
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K+1):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      # Take copy of biases  We'll change one element each time\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    raise RuntimeError(\"Failure!  Derivatives different.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5bd67b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cb5bd67b"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a63e12",
      "metadata": {
        "id": "e5a63e12"
      },
      "source": [
        "---\n",
        "\n",
        "End of notebook.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "otter": {
      "OK_FORMAT": true,
      "assignment_name": "backpropagation",
      "tests": {
        "q1.1": {
          "name": "q1.1",
          "points": 5,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert np.allclose(f0, 1.23, atol=0.001)\n>>> assert np.allclose(h1, 0.942, atol=0.001)\n>>> assert np.allclose(f1, 1.623, atol=0.001)\n>>> assert np.allclose(h2, 5.068, atol=0.001)\n>>> assert np.allclose(f2, 7.137, atol=0.001)\n>>> assert np.allclose(h3, 0.657, atol=0.001)\n>>> assert np.allclose(f3, 2.372, atol=0.001)\n>>> assert np.allclose(l_i, l_i_func, atol=0.001)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q1.2": {
          "name": "q1.2",
          "points": 5,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert np.allclose(dldf3, 0.745, atol=0.001)\n>>> assert np.allclose(dldh3, 2.234, atol=0.001)\n>>> assert np.allclose(dldf2, -1.683, atol=0.001)\n>>> assert np.allclose(dldh2, -3.366, atol=0.001)\n>>> assert np.allclose(dldf1, -17.06, atol=0.001)\n>>> assert np.allclose(dldh1, 6.824, atol=0.001)\n>>> assert np.allclose(dldf0, 2.281, atol=0.001)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q1.3": {
          "name": "q1.3",
          "points": 5,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert np.allclose(dldbeta3, 0.745, atol=0.001)\n>>> assert np.allclose(dldomega3, 0.489, atol=0.001)\n>>> assert np.allclose(dldbeta2, -1.683, atol=0.001)\n>>> assert np.allclose(dldomega2, -8.53, atol=0.001)\n>>> assert np.allclose(dldbeta1, -17.06, atol=0.001)\n>>> assert np.allclose(dldomega1, -16.079, atol=0.001)\n>>> assert np.allclose(dldbeta0, 2.281, atol=0.001)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q2.1": {
          "name": "q2.1",
          "points": 10,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert np.allclose(net_output[0, 0], 1.907, atol=0.001)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "q2.2": {
          "name": "q2.2",
          "points": 10,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> np.set_printoptions(precision=3)\n>>> all_dl_dweights_fd = [None] * (K + 1)\n>>> all_dl_dbiases_fd = [None] * (K + 1)\n>>> delta_fd = 1e-06\n>>> for layer in range(K + 1):\n...     dl_dbias = np.zeros_like(all_dl_dbiases[layer])\n...     for row in range(all_biases[layer].shape[0]):\n...         all_biases_copy = [np.array(x) for x in all_biases]\n...         all_biases_copy[layer][row] += delta_fd\n...         network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n...         network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n...         dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2, y)) / delta_fd\n...     all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n...     assert np.allclose(all_dl_dbiases_fd[layer], all_dl_dbiases[layer], rtol=1e-05, atol=1e-08, equal_nan=False)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> for layer in range(K + 1):\n...     dl_dweight = np.zeros_like(all_dl_dweights[layer])\n...     for row in range(all_weights[layer].shape[0]):\n...         for col in range(all_weights[layer].shape[1]):\n...             all_weights_copy = [np.array(x) for x in all_weights]\n...             all_weights_copy[layer][row][col] += delta_fd\n...             network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n...             network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n...             dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2, y)) / delta_fd\n...     all_dl_dweights_fd[layer] = np.array(dl_dweight)\n...     assert np.allclose(all_dl_dweights_fd[layer], all_dl_dweights[layer], rtol=1e-05, atol=1e-08, equal_nan=False)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}